{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid_RNN_GRU (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkq1KMz3gP0O",
        "colab_type": "text"
      },
      "source": [
        "#Predicting Covid-19 Cases Using RNN GRU Model\n",
        "\n",
        "This use-case provides a demo of how RNN and GRU can be used to model time series data. In our use-case, we will use Covid-19 data to predict the number of future cases based on the available historical data. \n",
        "\n",
        "Workflow:\n",
        "\n",
        "\n",
        "1.   Understanding the problem\n",
        "2.   Collecting the data\n",
        "3.   Data preprocessing\n",
        "4.   Build Model using RNN & Grated Recurrent Unit (GRU)\n",
        "5.   Training the dataset\n",
        "6.   Predict \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY2VSBddhV9V",
        "colab_type": "text"
      },
      "source": [
        "##1. Understanding the problem\n",
        "You have been given a dataset that has data of cases related to the spread of Covid-19 virus in India. We have to model the time series data using RNN GRU sequential model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH5FYBPGh1O3",
        "colab_type": "text"
      },
      "source": [
        "###Import the necessary libraries\n",
        "\n",
        "We will use keras for this purpose. Please load the following packages before you proceed further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c092IzdngTQx",
        "colab_type": "code",
        "outputId": "5be61a89-32a6-4060-e3a8-c578832f7d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.dates as mdates\n",
        "import datetime as dt\n",
        "\n",
        "\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.models import Sequential, load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g9qNnBsh_1_",
        "colab_type": "text"
      },
      "source": [
        "##2. Collecting the Data\n",
        "The dataset covid19.csv will be used for the demo. The dataset has the covid-19 cases recorded in India. It has been downloaded from the site https://www.kaggle.com/imdevskp/covid19-corona-virus-india-dataset \n",
        "\n",
        "The dataset has been modified to just keep 2 columns that are relevant to our analysis. It contains 2 fields and 89 rows:\n",
        "\n",
        "The 5 columns in the dataset are:\n",
        "\n",
        "1.   Date - Date of the cases starting from 31-Jan-2020. \n",
        "2.   Cases - Number of cases registered.\n",
        "\n",
        "\n",
        "Please note that the availablity of data is scarse. \n",
        "\n",
        "Load the dataset using pandas read_csv function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWip1W4QyRIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main_dataset = pd.read_csv(\"covid.csv\") #Read the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0TZHFpVEGI1",
        "colab_type": "code",
        "outputId": "2d8dd11e-b441-4f1b-b85e-1650842ed41c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "main_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Cases</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/31/2020</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2/1/2020</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2/2/2020</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2/3/2020</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2/4/2020</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>4/24/2020</td>\n",
              "      <td>1684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>4/25/2020</td>\n",
              "      <td>1429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>4/26/2020</td>\n",
              "      <td>1586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>4/27/2020</td>\n",
              "      <td>1568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>4/28/2020</td>\n",
              "      <td>1543</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  Cases\n",
              "0   1/31/2020      0\n",
              "1    2/1/2020      1\n",
              "2    2/2/2020      1\n",
              "3    2/3/2020      0\n",
              "4    2/4/2020      0\n",
              "..        ...    ...\n",
              "84  4/24/2020   1684\n",
              "85  4/25/2020   1429\n",
              "86  4/26/2020   1586\n",
              "87  4/27/2020   1568\n",
              "88  4/28/2020   1543\n",
              "\n",
              "[89 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdM6CsxNuVgh",
        "colab_type": "text"
      },
      "source": [
        "###Trend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW9BETi_uEBF",
        "colab_type": "code",
        "outputId": "8ede89dd-f76e-4b0c-f964-c645c29939c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "plt.plot(main_dataset.Date,main_dataset.Cases)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qknFf0z0HaI",
        "colab_type": "text"
      },
      "source": [
        "##3. Data Preprocessing\n",
        "\n",
        "We use following steps to preprocess the available dataset.\n",
        "\n",
        "\n",
        "\n",
        "1.   Create new features from the available variables\n",
        "2.   Create target and predictor datasets\n",
        "3.   Scale the values in the datasets\n",
        "4.   Get the datasets into desired format and shape.\n",
        "5.   Split the dataset into train & test with appropriate ratio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMwmojSe5i-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = main_dataset #Create another dataset same as the main dataset. Just to avoid replacing the main one. \n",
        "dataset = dataset.set_index('Date') #Set the index of the dataset as the date\n",
        "dataset = dataset['Cases'] #We will be left with only one column Cases for our dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFZsAHbykFc0",
        "colab_type": "text"
      },
      "source": [
        "Create a new variable called Moving Average. Moving Average is a calculation of averages of subsequent datapoints. Example:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> 1-Jan-2010   100\n",
        "\n",
        "\n",
        "> 2-Jan-2010   200\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Moving average will be (200+100)/2 = 150\n",
        "\n",
        "We will take moving average for 3 days in our example.\n",
        "\n",
        "Note: You can create any number of such variables and any days."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbdzMl4gL_5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset=dataset.to_frame() #Convert the Series into a dataframe object\n",
        "dataset['Cases_MA2']=dataset.expanding(min_periods=3).mean()  #min_periods is used to set the no. of periods for calculating MA. Mean is for calculating average. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIirZWKilJ2b",
        "colab_type": "text"
      },
      "source": [
        "Create a new variable called Exponential Moving Average(EMA). EMA gives more weightage to the recent values rather than the old values. This could be handy in time series forecasting techniques. \n",
        "\n",
        "We will take exponential moving average for 3 days in our example.\n",
        "\n",
        "Note: You can create any number of such variables and any days."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3lJtf9bWRsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['EMA'] = dataset.iloc[:,0].ewm(span=3,adjust=False).mean() #ewm is used exponential moving average"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf6gysbdWSan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset[dataset['Cases_MA2'].notna()] #Remove the rows that have NAs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjeZSygkmNZp",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoqB0BfFe26l",
        "colab_type": "code",
        "outputId": "847815c3-3e89-49b6-9bf9-a8f5b6a46ecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cases</th>\n",
              "      <th>Cases_MA2</th>\n",
              "      <th>EMA</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2/2/2020</th>\n",
              "      <td>1</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2/3/2020</th>\n",
              "      <td>0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2/4/2020</th>\n",
              "      <td>0</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.187500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2/5/2020</th>\n",
              "      <td>0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.093750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2/6/2020</th>\n",
              "      <td>0</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.046875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Cases  Cases_MA2       EMA\n",
              "Date                                \n",
              "2/2/2020      1   0.666667  0.750000\n",
              "2/3/2020      0   0.500000  0.375000\n",
              "2/4/2020      0   0.400000  0.187500\n",
              "2/5/2020      0   0.333333  0.093750\n",
              "2/6/2020      0   0.285714  0.046875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIMN7E2PsX2_",
        "colab_type": "text"
      },
      "source": [
        "Here we will be predicting the column 'Cases'. Hence the target variable will be 'Cases'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ_gtFHNyUZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cases is the column we have to predict. \n",
        "target=dataset[['Cases']]\n",
        "del dataset['Cases']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQK7tbKRwj-b",
        "colab_type": "text"
      },
      "source": [
        "Adjust the shapes of predictor and target sets. For this, we remove the last row of both the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-13AY5Ig45lk",
        "colab_type": "code",
        "outputId": "3ada2049-0e6f-4e6e-fced-8604b4167f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "dataset.drop(dataset.index[len(dataset)-1], axis=0, inplace=True) #Drop the last row in dataset\n",
        "target.drop(target.index[len(target)-1], axis=0, inplace=True) #Drop the last row in target"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgT5js03xHYP",
        "colab_type": "text"
      },
      "source": [
        "Convert both dataset and target into numpy arrays.\n",
        "\n",
        "\n",
        "*   x -> dataset with predictor variables\n",
        "*   y -> dataset with target variable\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvamkbLRyoo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = dataset.values, target.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS2E3yDixWeW",
        "colab_type": "text"
      },
      "source": [
        "We now scale the values for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJLucXSoy8XT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_scaled_set = MinMaxScaler() #Use MinMaxScaler function from sklearn.preprocessing library \n",
        "y_scaled_set = MinMaxScaler()\n",
        "\n",
        "X = x_scaled_set.fit_transform(x) #Fit to data and transform the data array x\n",
        "Y = y_scaled_set.fit_transform(y.reshape(-1,1)) #Fit to data and transform the data array y. Reshape the y variable to long format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g67PWyZlmXNA",
        "colab_type": "text"
      },
      "source": [
        "Split the X into train and test datasets.\n",
        "\n",
        "Consider Training data as first 77 rows of the data and next 8 rows will be used for testing the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N35kEGyr143x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_data=X[0:77,]\n",
        "y_train_data=Y[0:77,]\n",
        "X_test_data=X[78:86,]\n",
        "y_test_data=Y[78:86,]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j__I7eximeXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_data = X_train_data.reshape((-1,1,2)) #Reshape train dataset to required format\n",
        "X_test_data = X_test_data.reshape((-1,1,2)) #Reshape test dataset to required format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmnPYBJq08np",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build Model using RNN & Grated Recurrent Unit (GRU)\n",
        "\n",
        "We will use the Sequential class from keras.models library to define a linear stack of network layers. These form a model. \n",
        "\n",
        "Using the add method, the necessary layers would be added to the model.\n",
        "\n",
        "RNNs have an issue of short-term memory which means if the sequence is long, they tend to leave leave out the earlier ones and only retain information related to the later ones. To remove this problem, GRUs are used. In a way, GRU helps in keeping only the important information in the whole sequence. Initially, we add the GRU layer with necessary parameters. \n",
        "\n",
        "Then, we add a Dropout layer to prevent overfitting of the model.\n",
        "\n",
        "We add another set of GRU and Dropout layers.\n",
        "\n",
        "Post which we add a Dense layer which gives us the fully connected layer. We use the regular sigmoid activation function.\n",
        "\n",
        "In our model, we will use Adam optimizer and consider mean square error as our loss function since ours is a Regression model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ps_hdPIzkee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GRU_model = Sequential() #Use Sequential function from keras.model library\n",
        "GRU_model.add(GRU(units=512,\n",
        "              return_sequences=True,\n",
        "              input_shape=(1, 2))) #Add GRU model with 512 units. Note that return sequences should be true to pass on the sequences. Ensure that input_shape is inline with the inout data. \n",
        "GRU_model.add(Dropout(0.2)) #Add a dropout layer to avoid overfitting.\n",
        "GRU_model.add(GRU(units=256)) #Add another GRU layer with 256 units. \n",
        "GRU_model.add(Dropout(0.2)) #Add another dropout layer to avoid overfitting. \n",
        "GRU_model.add(Dense(1, activation='sigmoid')) #Finally, add a dense layer for getting a fully connected layer. \n",
        "GRU_model.compile(loss='mse', optimizer='adam') #mse is mean square error which is the loss function and Adam optimizer will be the ooptimizer. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwrxiDXw-Sjz",
        "colab_type": "text"
      },
      "source": [
        "## 5. Training the Dataset\n",
        "\n",
        "Fit the model by passing the training data we created earlier along with relevant batch size, epochs, validation split. \n",
        "\n",
        "Batch Size - The batch size controls the number of training samples before the model's internal parameters are updated\n",
        "\n",
        "Epochs - How many times the whole training dataset be passed through the network. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju-M7vNb5-mj",
        "colab_type": "code",
        "outputId": "5d8715d8-0104-4d3c-ac24-c97e9aca37cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "GRU_model.fit(X_train_data,y_train_data,batch_size=250, epochs=500, validation_split=0.15, verbose=1) \n",
        "#GRU_model.save(\"covid_GRU.h5\") #Save the model if you want for reuse. "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 65 samples, validate on 12 samples\n",
            "Epoch 1/500\n",
            "65/65 [==============================] - 1s 20ms/step - loss: 0.2216 - val_loss: 0.0471\n",
            "Epoch 2/500\n",
            "65/65 [==============================] - 0s 912us/step - loss: 0.2185 - val_loss: 0.0483\n",
            "Epoch 3/500\n",
            "65/65 [==============================] - 0s 809us/step - loss: 0.2147 - val_loss: 0.0498\n",
            "Epoch 4/500\n",
            "65/65 [==============================] - 0s 813us/step - loss: 0.2103 - val_loss: 0.0516\n",
            "Epoch 5/500\n",
            "65/65 [==============================] - 0s 902us/step - loss: 0.2050 - val_loss: 0.0539\n",
            "Epoch 6/500\n",
            "65/65 [==============================] - 0s 882us/step - loss: 0.1987 - val_loss: 0.0568\n",
            "Epoch 7/500\n",
            "65/65 [==============================] - 0s 886us/step - loss: 0.1914 - val_loss: 0.0608\n",
            "Epoch 8/500\n",
            "65/65 [==============================] - 0s 901us/step - loss: 0.1830 - val_loss: 0.0659\n",
            "Epoch 9/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.1737 - val_loss: 0.0726\n",
            "Epoch 10/500\n",
            "65/65 [==============================] - 0s 913us/step - loss: 0.1634 - val_loss: 0.0811\n",
            "Epoch 11/500\n",
            "65/65 [==============================] - 0s 848us/step - loss: 0.1522 - val_loss: 0.0916\n",
            "Epoch 12/500\n",
            "65/65 [==============================] - 0s 851us/step - loss: 0.1401 - val_loss: 0.1044\n",
            "Epoch 13/500\n",
            "65/65 [==============================] - 0s 915us/step - loss: 0.1270 - val_loss: 0.1195\n",
            "Epoch 14/500\n",
            "65/65 [==============================] - 0s 893us/step - loss: 0.1132 - val_loss: 0.1367\n",
            "Epoch 15/500\n",
            "65/65 [==============================] - 0s 909us/step - loss: 0.1007 - val_loss: 0.1555\n",
            "Epoch 16/500\n",
            "65/65 [==============================] - 0s 894us/step - loss: 0.0869 - val_loss: 0.1752\n",
            "Epoch 17/500\n",
            "65/65 [==============================] - 0s 894us/step - loss: 0.0741 - val_loss: 0.1950\n",
            "Epoch 18/500\n",
            "65/65 [==============================] - 0s 923us/step - loss: 0.0611 - val_loss: 0.2139\n",
            "Epoch 19/500\n",
            "65/65 [==============================] - 0s 942us/step - loss: 0.0520 - val_loss: 0.2312\n",
            "Epoch 20/500\n",
            "65/65 [==============================] - 0s 915us/step - loss: 0.0413 - val_loss: 0.2463\n",
            "Epoch 21/500\n",
            "65/65 [==============================] - 0s 924us/step - loss: 0.0334 - val_loss: 0.2588\n",
            "Epoch 22/500\n",
            "65/65 [==============================] - 0s 927us/step - loss: 0.0265 - val_loss: 0.2691\n",
            "Epoch 23/500\n",
            "65/65 [==============================] - 0s 862us/step - loss: 0.0215 - val_loss: 0.2773\n",
            "Epoch 24/500\n",
            "65/65 [==============================] - 0s 937us/step - loss: 0.0174 - val_loss: 0.2837\n",
            "Epoch 25/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.2885\n",
            "Epoch 26/500\n",
            "65/65 [==============================] - 0s 881us/step - loss: 0.0128 - val_loss: 0.2923\n",
            "Epoch 27/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 0.0110 - val_loss: 0.2951\n",
            "Epoch 28/500\n",
            "65/65 [==============================] - 0s 856us/step - loss: 0.0102 - val_loss: 0.2973\n",
            "Epoch 29/500\n",
            "65/65 [==============================] - 0s 856us/step - loss: 0.0093 - val_loss: 0.2989\n",
            "Epoch 30/500\n",
            "65/65 [==============================] - 0s 871us/step - loss: 0.0088 - val_loss: 0.3002\n",
            "Epoch 31/500\n",
            "65/65 [==============================] - 0s 908us/step - loss: 0.0087 - val_loss: 0.3011\n",
            "Epoch 32/500\n",
            "65/65 [==============================] - 0s 907us/step - loss: 0.0083 - val_loss: 0.3019\n",
            "Epoch 33/500\n",
            "65/65 [==============================] - 0s 920us/step - loss: 0.0082 - val_loss: 0.3025\n",
            "Epoch 34/500\n",
            "65/65 [==============================] - 0s 868us/step - loss: 0.0083 - val_loss: 0.3029\n",
            "Epoch 35/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 0.0082 - val_loss: 0.3033\n",
            "Epoch 36/500\n",
            "65/65 [==============================] - 0s 865us/step - loss: 0.0083 - val_loss: 0.3036\n",
            "Epoch 37/500\n",
            "65/65 [==============================] - 0s 987us/step - loss: 0.0082 - val_loss: 0.3038\n",
            "Epoch 38/500\n",
            "65/65 [==============================] - 0s 875us/step - loss: 0.0083 - val_loss: 0.3040\n",
            "Epoch 39/500\n",
            "65/65 [==============================] - 0s 873us/step - loss: 0.0082 - val_loss: 0.3042\n",
            "Epoch 40/500\n",
            "65/65 [==============================] - 0s 887us/step - loss: 0.0083 - val_loss: 0.3043\n",
            "Epoch 41/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.3044\n",
            "Epoch 42/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.3045\n",
            "Epoch 43/500\n",
            "65/65 [==============================] - 0s 891us/step - loss: 0.0083 - val_loss: 0.3046\n",
            "Epoch 44/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 0.3046\n",
            "Epoch 45/500\n",
            "65/65 [==============================] - 0s 912us/step - loss: 0.0083 - val_loss: 0.3047\n",
            "Epoch 46/500\n",
            "65/65 [==============================] - 0s 897us/step - loss: 0.0083 - val_loss: 0.3047\n",
            "Epoch 47/500\n",
            "65/65 [==============================] - 0s 934us/step - loss: 0.0083 - val_loss: 0.3047\n",
            "Epoch 48/500\n",
            "65/65 [==============================] - 0s 855us/step - loss: 0.0083 - val_loss: 0.3047\n",
            "Epoch 49/500\n",
            "65/65 [==============================] - 0s 844us/step - loss: 0.0083 - val_loss: 0.3047\n",
            "Epoch 50/500\n",
            "65/65 [==============================] - 0s 834us/step - loss: 0.0083 - val_loss: 0.3047\n",
            "Epoch 51/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 0.0083 - val_loss: 0.3047\n",
            "Epoch 52/500\n",
            "65/65 [==============================] - 0s 843us/step - loss: 0.0083 - val_loss: 0.3047\n",
            "Epoch 53/500\n",
            "65/65 [==============================] - 0s 878us/step - loss: 0.0083 - val_loss: 0.3046\n",
            "Epoch 54/500\n",
            "65/65 [==============================] - 0s 862us/step - loss: 0.0083 - val_loss: 0.3046\n",
            "Epoch 55/500\n",
            "65/65 [==============================] - 0s 873us/step - loss: 0.0083 - val_loss: 0.3046\n",
            "Epoch 56/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 0.0083 - val_loss: 0.3045\n",
            "Epoch 57/500\n",
            "65/65 [==============================] - 0s 863us/step - loss: 0.0083 - val_loss: 0.3045\n",
            "Epoch 58/500\n",
            "65/65 [==============================] - 0s 870us/step - loss: 0.0083 - val_loss: 0.3044\n",
            "Epoch 59/500\n",
            "65/65 [==============================] - 0s 894us/step - loss: 0.0083 - val_loss: 0.3043\n",
            "Epoch 60/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 0.3043\n",
            "Epoch 61/500\n",
            "65/65 [==============================] - 0s 955us/step - loss: 0.0083 - val_loss: 0.3042\n",
            "Epoch 62/500\n",
            "65/65 [==============================] - 0s 856us/step - loss: 0.0083 - val_loss: 0.3041\n",
            "Epoch 63/500\n",
            "65/65 [==============================] - 0s 884us/step - loss: 0.0084 - val_loss: 0.3040\n",
            "Epoch 64/500\n",
            "65/65 [==============================] - 0s 891us/step - loss: 0.0083 - val_loss: 0.3039\n",
            "Epoch 65/500\n",
            "65/65 [==============================] - 0s 939us/step - loss: 0.0083 - val_loss: 0.3038\n",
            "Epoch 66/500\n",
            "65/65 [==============================] - 0s 878us/step - loss: 0.0083 - val_loss: 0.3037\n",
            "Epoch 67/500\n",
            "65/65 [==============================] - 0s 892us/step - loss: 0.0083 - val_loss: 0.3036\n",
            "Epoch 68/500\n",
            "65/65 [==============================] - 0s 891us/step - loss: 0.0082 - val_loss: 0.3035\n",
            "Epoch 69/500\n",
            "65/65 [==============================] - 0s 924us/step - loss: 0.0083 - val_loss: 0.3033\n",
            "Epoch 70/500\n",
            "65/65 [==============================] - 0s 969us/step - loss: 0.0082 - val_loss: 0.3032\n",
            "Epoch 71/500\n",
            "65/65 [==============================] - 0s 921us/step - loss: 0.0082 - val_loss: 0.3030\n",
            "Epoch 72/500\n",
            "65/65 [==============================] - 0s 878us/step - loss: 0.0083 - val_loss: 0.3028\n",
            "Epoch 73/500\n",
            "65/65 [==============================] - 0s 964us/step - loss: 0.0082 - val_loss: 0.3027\n",
            "Epoch 74/500\n",
            "65/65 [==============================] - 0s 884us/step - loss: 0.0082 - val_loss: 0.3025\n",
            "Epoch 75/500\n",
            "65/65 [==============================] - 0s 887us/step - loss: 0.0082 - val_loss: 0.3022\n",
            "Epoch 76/500\n",
            "65/65 [==============================] - 0s 958us/step - loss: 0.0082 - val_loss: 0.3020\n",
            "Epoch 77/500\n",
            "65/65 [==============================] - 0s 905us/step - loss: 0.0082 - val_loss: 0.3017\n",
            "Epoch 78/500\n",
            "65/65 [==============================] - 0s 874us/step - loss: 0.0082 - val_loss: 0.3015\n",
            "Epoch 79/500\n",
            "65/65 [==============================] - 0s 940us/step - loss: 0.0081 - val_loss: 0.3011\n",
            "Epoch 80/500\n",
            "65/65 [==============================] - 0s 904us/step - loss: 0.0082 - val_loss: 0.3008\n",
            "Epoch 81/500\n",
            "65/65 [==============================] - 0s 881us/step - loss: 0.0081 - val_loss: 0.3005\n",
            "Epoch 82/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 0.0080 - val_loss: 0.3000\n",
            "Epoch 83/500\n",
            "65/65 [==============================] - 0s 857us/step - loss: 0.0082 - val_loss: 0.2996\n",
            "Epoch 84/500\n",
            "65/65 [==============================] - 0s 842us/step - loss: 0.0081 - val_loss: 0.2991\n",
            "Epoch 85/500\n",
            "65/65 [==============================] - 0s 831us/step - loss: 0.0081 - val_loss: 0.2986\n",
            "Epoch 86/500\n",
            "65/65 [==============================] - 0s 878us/step - loss: 0.0081 - val_loss: 0.2980\n",
            "Epoch 87/500\n",
            "65/65 [==============================] - 0s 845us/step - loss: 0.0080 - val_loss: 0.2974\n",
            "Epoch 88/500\n",
            "65/65 [==============================] - 0s 831us/step - loss: 0.0080 - val_loss: 0.2967\n",
            "Epoch 89/500\n",
            "65/65 [==============================] - 0s 848us/step - loss: 0.0079 - val_loss: 0.2959\n",
            "Epoch 90/500\n",
            "65/65 [==============================] - 0s 937us/step - loss: 0.0080 - val_loss: 0.2950\n",
            "Epoch 91/500\n",
            "65/65 [==============================] - 0s 857us/step - loss: 0.0079 - val_loss: 0.2940\n",
            "Epoch 92/500\n",
            "65/65 [==============================] - 0s 907us/step - loss: 0.0080 - val_loss: 0.2929\n",
            "Epoch 93/500\n",
            "65/65 [==============================] - 0s 884us/step - loss: 0.0079 - val_loss: 0.2917\n",
            "Epoch 94/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0079 - val_loss: 0.2903\n",
            "Epoch 95/500\n",
            "65/65 [==============================] - 0s 884us/step - loss: 0.0079 - val_loss: 0.2888\n",
            "Epoch 96/500\n",
            "65/65 [==============================] - 0s 920us/step - loss: 0.0079 - val_loss: 0.2870\n",
            "Epoch 97/500\n",
            "65/65 [==============================] - 0s 865us/step - loss: 0.0077 - val_loss: 0.2850\n",
            "Epoch 98/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 0.0078 - val_loss: 0.2827\n",
            "Epoch 99/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0078 - val_loss: 0.2801\n",
            "Epoch 100/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0077 - val_loss: 0.2771\n",
            "Epoch 101/500\n",
            "65/65 [==============================] - 0s 889us/step - loss: 0.0076 - val_loss: 0.2737\n",
            "Epoch 102/500\n",
            "65/65 [==============================] - 0s 868us/step - loss: 0.0075 - val_loss: 0.2698\n",
            "Epoch 103/500\n",
            "65/65 [==============================] - 0s 931us/step - loss: 0.0075 - val_loss: 0.2653\n",
            "Epoch 104/500\n",
            "65/65 [==============================] - 0s 848us/step - loss: 0.0075 - val_loss: 0.2600\n",
            "Epoch 105/500\n",
            "65/65 [==============================] - 0s 893us/step - loss: 0.0074 - val_loss: 0.2540\n",
            "Epoch 106/500\n",
            "65/65 [==============================] - 0s 892us/step - loss: 0.0073 - val_loss: 0.2471\n",
            "Epoch 107/500\n",
            "65/65 [==============================] - 0s 876us/step - loss: 0.0071 - val_loss: 0.2390\n",
            "Epoch 108/500\n",
            "65/65 [==============================] - 0s 847us/step - loss: 0.0071 - val_loss: 0.2297\n",
            "Epoch 109/500\n",
            "65/65 [==============================] - 0s 858us/step - loss: 0.0070 - val_loss: 0.2190\n",
            "Epoch 110/500\n",
            "65/65 [==============================] - 0s 866us/step - loss: 0.0070 - val_loss: 0.2068\n",
            "Epoch 111/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0067 - val_loss: 0.1929\n",
            "Epoch 112/500\n",
            "65/65 [==============================] - 0s 947us/step - loss: 0.0068 - val_loss: 0.1776\n",
            "Epoch 113/500\n",
            "65/65 [==============================] - 0s 868us/step - loss: 0.0065 - val_loss: 0.1607\n",
            "Epoch 114/500\n",
            "65/65 [==============================] - 0s 853us/step - loss: 0.0061 - val_loss: 0.1424\n",
            "Epoch 115/500\n",
            "65/65 [==============================] - 0s 843us/step - loss: 0.0062 - val_loss: 0.1233\n",
            "Epoch 116/500\n",
            "65/65 [==============================] - 0s 850us/step - loss: 0.0058 - val_loss: 0.1039\n",
            "Epoch 117/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 0.0057 - val_loss: 0.0850\n",
            "Epoch 118/500\n",
            "65/65 [==============================] - 0s 858us/step - loss: 0.0056 - val_loss: 0.0677\n",
            "Epoch 119/500\n",
            "65/65 [==============================] - 0s 974us/step - loss: 0.0053 - val_loss: 0.0526\n",
            "Epoch 120/500\n",
            "65/65 [==============================] - 0s 938us/step - loss: 0.0048 - val_loss: 0.0406\n",
            "Epoch 121/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 0.0043 - val_loss: 0.0323\n",
            "Epoch 122/500\n",
            "65/65 [==============================] - 0s 883us/step - loss: 0.0042 - val_loss: 0.0278\n",
            "Epoch 123/500\n",
            "65/65 [==============================] - 0s 886us/step - loss: 0.0037 - val_loss: 0.0271\n",
            "Epoch 124/500\n",
            "65/65 [==============================] - 0s 890us/step - loss: 0.0034 - val_loss: 0.0299\n",
            "Epoch 125/500\n",
            "65/65 [==============================] - 0s 905us/step - loss: 0.0030 - val_loss: 0.0355\n",
            "Epoch 126/500\n",
            "65/65 [==============================] - 0s 912us/step - loss: 0.0028 - val_loss: 0.0434\n",
            "Epoch 127/500\n",
            "65/65 [==============================] - 0s 930us/step - loss: 0.0023 - val_loss: 0.0529\n",
            "Epoch 128/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0019 - val_loss: 0.0634\n",
            "Epoch 129/500\n",
            "65/65 [==============================] - 0s 927us/step - loss: 0.0016 - val_loss: 0.0744\n",
            "Epoch 130/500\n",
            "65/65 [==============================] - 0s 910us/step - loss: 0.0014 - val_loss: 0.0853\n",
            "Epoch 131/500\n",
            "65/65 [==============================] - 0s 867us/step - loss: 0.0014 - val_loss: 0.0960\n",
            "Epoch 132/500\n",
            "65/65 [==============================] - 0s 893us/step - loss: 0.0011 - val_loss: 0.1059\n",
            "Epoch 133/500\n",
            "65/65 [==============================] - 0s 856us/step - loss: 0.0011 - val_loss: 0.1147\n",
            "Epoch 134/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 0.0012 - val_loss: 0.1224\n",
            "Epoch 135/500\n",
            "65/65 [==============================] - 0s 860us/step - loss: 0.0014 - val_loss: 0.1288\n",
            "Epoch 136/500\n",
            "65/65 [==============================] - 0s 831us/step - loss: 0.0012 - val_loss: 0.1339\n",
            "Epoch 137/500\n",
            "65/65 [==============================] - 0s 878us/step - loss: 0.0015 - val_loss: 0.1377\n",
            "Epoch 138/500\n",
            "65/65 [==============================] - 0s 870us/step - loss: 0.0016 - val_loss: 0.1399\n",
            "Epoch 139/500\n",
            "65/65 [==============================] - 0s 898us/step - loss: 0.0016 - val_loss: 0.1408\n",
            "Epoch 140/500\n",
            "65/65 [==============================] - 0s 884us/step - loss: 0.0014 - val_loss: 0.1407\n",
            "Epoch 141/500\n",
            "65/65 [==============================] - 0s 869us/step - loss: 0.0015 - val_loss: 0.1396\n",
            "Epoch 142/500\n",
            "65/65 [==============================] - 0s 838us/step - loss: 0.0015 - val_loss: 0.1377\n",
            "Epoch 143/500\n",
            "65/65 [==============================] - 0s 865us/step - loss: 0.0015 - val_loss: 0.1352\n",
            "Epoch 144/500\n",
            "65/65 [==============================] - 0s 901us/step - loss: 0.0014 - val_loss: 0.1321\n",
            "Epoch 145/500\n",
            "65/65 [==============================] - 0s 897us/step - loss: 0.0013 - val_loss: 0.1287\n",
            "Epoch 146/500\n",
            "65/65 [==============================] - 0s 848us/step - loss: 0.0014 - val_loss: 0.1249\n",
            "Epoch 147/500\n",
            "65/65 [==============================] - 0s 881us/step - loss: 0.0013 - val_loss: 0.1210\n",
            "Epoch 148/500\n",
            "65/65 [==============================] - 0s 967us/step - loss: 0.0012 - val_loss: 0.1169\n",
            "Epoch 149/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.1132\n",
            "Epoch 150/500\n",
            "65/65 [==============================] - 0s 869us/step - loss: 0.0011 - val_loss: 0.1096\n",
            "Epoch 151/500\n",
            "65/65 [==============================] - 0s 874us/step - loss: 0.0010 - val_loss: 0.1063\n",
            "Epoch 152/500\n",
            "65/65 [==============================] - 0s 880us/step - loss: 0.0010 - val_loss: 0.1034\n",
            "Epoch 153/500\n",
            "65/65 [==============================] - 0s 856us/step - loss: 0.0010 - val_loss: 0.1011\n",
            "Epoch 154/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 0.0012 - val_loss: 0.0993\n",
            "Epoch 155/500\n",
            "65/65 [==============================] - 0s 877us/step - loss: 0.0011 - val_loss: 0.0979\n",
            "Epoch 156/500\n",
            "65/65 [==============================] - 0s 971us/step - loss: 0.0011 - val_loss: 0.0973\n",
            "Epoch 157/500\n",
            "65/65 [==============================] - 0s 891us/step - loss: 0.0011 - val_loss: 0.0972\n",
            "Epoch 158/500\n",
            "65/65 [==============================] - 0s 892us/step - loss: 0.0012 - val_loss: 0.0975\n",
            "Epoch 159/500\n",
            "65/65 [==============================] - 0s 895us/step - loss: 0.0012 - val_loss: 0.0981\n",
            "Epoch 160/500\n",
            "65/65 [==============================] - 0s 871us/step - loss: 0.0012 - val_loss: 0.0991\n",
            "Epoch 161/500\n",
            "65/65 [==============================] - 0s 956us/step - loss: 0.0011 - val_loss: 0.1004\n",
            "Epoch 162/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.1021\n",
            "Epoch 163/500\n",
            "65/65 [==============================] - 0s 932us/step - loss: 0.0011 - val_loss: 0.1039\n",
            "Epoch 164/500\n",
            "65/65 [==============================] - 0s 858us/step - loss: 0.0011 - val_loss: 0.1057\n",
            "Epoch 165/500\n",
            "65/65 [==============================] - 0s 863us/step - loss: 0.0011 - val_loss: 0.1076\n",
            "Epoch 166/500\n",
            "65/65 [==============================] - 0s 860us/step - loss: 9.8459e-04 - val_loss: 0.1094\n",
            "Epoch 167/500\n",
            "65/65 [==============================] - 0s 904us/step - loss: 0.0011 - val_loss: 0.1110\n",
            "Epoch 168/500\n",
            "65/65 [==============================] - 0s 813us/step - loss: 9.7598e-04 - val_loss: 0.1126\n",
            "Epoch 169/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 0.0011 - val_loss: 0.1141\n",
            "Epoch 170/500\n",
            "65/65 [==============================] - 0s 877us/step - loss: 9.4368e-04 - val_loss: 0.1156\n",
            "Epoch 171/500\n",
            "65/65 [==============================] - 0s 863us/step - loss: 0.0011 - val_loss: 0.1170\n",
            "Epoch 172/500\n",
            "65/65 [==============================] - 0s 865us/step - loss: 0.0011 - val_loss: 0.1183\n",
            "Epoch 173/500\n",
            "65/65 [==============================] - 0s 886us/step - loss: 0.0010 - val_loss: 0.1193\n",
            "Epoch 174/500\n",
            "65/65 [==============================] - 0s 907us/step - loss: 9.8800e-04 - val_loss: 0.1199\n",
            "Epoch 175/500\n",
            "65/65 [==============================] - 0s 845us/step - loss: 9.7323e-04 - val_loss: 0.1202\n",
            "Epoch 176/500\n",
            "65/65 [==============================] - 0s 886us/step - loss: 0.0011 - val_loss: 0.1202\n",
            "Epoch 177/500\n",
            "65/65 [==============================] - 0s 917us/step - loss: 0.0010 - val_loss: 0.1201\n",
            "Epoch 178/500\n",
            "65/65 [==============================] - 0s 862us/step - loss: 0.0011 - val_loss: 0.1199\n",
            "Epoch 179/500\n",
            "65/65 [==============================] - 0s 887us/step - loss: 0.0011 - val_loss: 0.1195\n",
            "Epoch 180/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.9721e-04 - val_loss: 0.1190\n",
            "Epoch 181/500\n",
            "65/65 [==============================] - 0s 886us/step - loss: 0.0011 - val_loss: 0.1184\n",
            "Epoch 182/500\n",
            "65/65 [==============================] - 0s 902us/step - loss: 0.0011 - val_loss: 0.1178\n",
            "Epoch 183/500\n",
            "65/65 [==============================] - 0s 909us/step - loss: 0.0011 - val_loss: 0.1170\n",
            "Epoch 184/500\n",
            "65/65 [==============================] - 0s 915us/step - loss: 0.0010 - val_loss: 0.1163\n",
            "Epoch 185/500\n",
            "65/65 [==============================] - 0s 922us/step - loss: 0.0010 - val_loss: 0.1156\n",
            "Epoch 186/500\n",
            "65/65 [==============================] - 0s 870us/step - loss: 9.6180e-04 - val_loss: 0.1149\n",
            "Epoch 187/500\n",
            "65/65 [==============================] - 0s 871us/step - loss: 9.8492e-04 - val_loss: 0.1145\n",
            "Epoch 188/500\n",
            "65/65 [==============================] - 0s 984us/step - loss: 9.8218e-04 - val_loss: 0.1142\n",
            "Epoch 189/500\n",
            "65/65 [==============================] - 0s 877us/step - loss: 0.0011 - val_loss: 0.1140\n",
            "Epoch 190/500\n",
            "65/65 [==============================] - 0s 874us/step - loss: 0.0011 - val_loss: 0.1138\n",
            "Epoch 191/500\n",
            "65/65 [==============================] - 0s 856us/step - loss: 0.0011 - val_loss: 0.1134\n",
            "Epoch 192/500\n",
            "65/65 [==============================] - 0s 855us/step - loss: 0.0011 - val_loss: 0.1132\n",
            "Epoch 193/500\n",
            "65/65 [==============================] - 0s 876us/step - loss: 0.0012 - val_loss: 0.1129\n",
            "Epoch 194/500\n",
            "65/65 [==============================] - 0s 895us/step - loss: 0.0010 - val_loss: 0.1125\n",
            "Epoch 195/500\n",
            "65/65 [==============================] - 0s 847us/step - loss: 9.5493e-04 - val_loss: 0.1120\n",
            "Epoch 196/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 0.0010 - val_loss: 0.1117\n",
            "Epoch 197/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.6651e-04 - val_loss: 0.1115\n",
            "Epoch 198/500\n",
            "65/65 [==============================] - 0s 911us/step - loss: 9.5445e-04 - val_loss: 0.1115\n",
            "Epoch 199/500\n",
            "65/65 [==============================] - 0s 887us/step - loss: 0.0010 - val_loss: 0.1117\n",
            "Epoch 200/500\n",
            "65/65 [==============================] - 0s 898us/step - loss: 0.0010 - val_loss: 0.1118\n",
            "Epoch 201/500\n",
            "65/65 [==============================] - 0s 872us/step - loss: 0.0010 - val_loss: 0.1120\n",
            "Epoch 202/500\n",
            "65/65 [==============================] - 0s 908us/step - loss: 9.2795e-04 - val_loss: 0.1123\n",
            "Epoch 203/500\n",
            "65/65 [==============================] - 0s 876us/step - loss: 0.0010 - val_loss: 0.1125\n",
            "Epoch 204/500\n",
            "65/65 [==============================] - 0s 888us/step - loss: 0.0010 - val_loss: 0.1127\n",
            "Epoch 205/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.6976e-04 - val_loss: 0.1130\n",
            "Epoch 206/500\n",
            "65/65 [==============================] - 0s 904us/step - loss: 9.9404e-04 - val_loss: 0.1132\n",
            "Epoch 207/500\n",
            "65/65 [==============================] - 0s 890us/step - loss: 9.5101e-04 - val_loss: 0.1135\n",
            "Epoch 208/500\n",
            "65/65 [==============================] - 0s 897us/step - loss: 9.2270e-04 - val_loss: 0.1137\n",
            "Epoch 209/500\n",
            "65/65 [==============================] - 0s 878us/step - loss: 0.0010 - val_loss: 0.1136\n",
            "Epoch 210/500\n",
            "65/65 [==============================] - 0s 889us/step - loss: 9.5170e-04 - val_loss: 0.1136\n",
            "Epoch 211/500\n",
            "65/65 [==============================] - 0s 891us/step - loss: 0.0010 - val_loss: 0.1136\n",
            "Epoch 212/500\n",
            "65/65 [==============================] - 0s 881us/step - loss: 0.0012 - val_loss: 0.1134\n",
            "Epoch 213/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 9.9870e-04 - val_loss: 0.1133\n",
            "Epoch 214/500\n",
            "65/65 [==============================] - 0s 990us/step - loss: 0.0011 - val_loss: 0.1131\n",
            "Epoch 215/500\n",
            "65/65 [==============================] - 0s 904us/step - loss: 8.9031e-04 - val_loss: 0.1131\n",
            "Epoch 216/500\n",
            "65/65 [==============================] - 0s 863us/step - loss: 0.0010 - val_loss: 0.1130\n",
            "Epoch 217/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 0.0011 - val_loss: 0.1129\n",
            "Epoch 218/500\n",
            "65/65 [==============================] - 0s 858us/step - loss: 0.0011 - val_loss: 0.1128\n",
            "Epoch 219/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 7.9489e-04 - val_loss: 0.1128\n",
            "Epoch 220/500\n",
            "65/65 [==============================] - 0s 851us/step - loss: 9.1025e-04 - val_loss: 0.1128\n",
            "Epoch 221/500\n",
            "65/65 [==============================] - 0s 841us/step - loss: 0.0010 - val_loss: 0.1128\n",
            "Epoch 222/500\n",
            "65/65 [==============================] - 0s 847us/step - loss: 9.8649e-04 - val_loss: 0.1127\n",
            "Epoch 223/500\n",
            "65/65 [==============================] - 0s 848us/step - loss: 9.7786e-04 - val_loss: 0.1126\n",
            "Epoch 224/500\n",
            "65/65 [==============================] - 0s 886us/step - loss: 0.0011 - val_loss: 0.1124\n",
            "Epoch 225/500\n",
            "65/65 [==============================] - 0s 880us/step - loss: 0.0010 - val_loss: 0.1123\n",
            "Epoch 226/500\n",
            "65/65 [==============================] - 0s 910us/step - loss: 8.5750e-04 - val_loss: 0.1121\n",
            "Epoch 227/500\n",
            "65/65 [==============================] - 0s 940us/step - loss: 9.6674e-04 - val_loss: 0.1118\n",
            "Epoch 228/500\n",
            "65/65 [==============================] - 0s 871us/step - loss: 0.0011 - val_loss: 0.1114\n",
            "Epoch 229/500\n",
            "65/65 [==============================] - 0s 857us/step - loss: 9.8486e-04 - val_loss: 0.1110\n",
            "Epoch 230/500\n",
            "65/65 [==============================] - 0s 850us/step - loss: 9.7143e-04 - val_loss: 0.1104\n",
            "Epoch 231/500\n",
            "65/65 [==============================] - 0s 913us/step - loss: 9.2342e-04 - val_loss: 0.1100\n",
            "Epoch 232/500\n",
            "65/65 [==============================] - 0s 858us/step - loss: 9.1469e-04 - val_loss: 0.1097\n",
            "Epoch 233/500\n",
            "65/65 [==============================] - 0s 906us/step - loss: 0.0010 - val_loss: 0.1094\n",
            "Epoch 234/500\n",
            "65/65 [==============================] - 0s 921us/step - loss: 9.6641e-04 - val_loss: 0.1093\n",
            "Epoch 235/500\n",
            "65/65 [==============================] - 0s 916us/step - loss: 0.0010 - val_loss: 0.1093\n",
            "Epoch 236/500\n",
            "65/65 [==============================] - 0s 868us/step - loss: 0.0011 - val_loss: 0.1094\n",
            "Epoch 237/500\n",
            "65/65 [==============================] - 0s 880us/step - loss: 8.7937e-04 - val_loss: 0.1093\n",
            "Epoch 238/500\n",
            "65/65 [==============================] - 0s 877us/step - loss: 0.0011 - val_loss: 0.1092\n",
            "Epoch 239/500\n",
            "65/65 [==============================] - 0s 853us/step - loss: 9.5460e-04 - val_loss: 0.1092\n",
            "Epoch 240/500\n",
            "65/65 [==============================] - 0s 850us/step - loss: 0.0012 - val_loss: 0.1090\n",
            "Epoch 241/500\n",
            "65/65 [==============================] - 0s 905us/step - loss: 9.8882e-04 - val_loss: 0.1089\n",
            "Epoch 242/500\n",
            "65/65 [==============================] - 0s 876us/step - loss: 0.0011 - val_loss: 0.1089\n",
            "Epoch 243/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.1087\n",
            "Epoch 244/500\n",
            "65/65 [==============================] - 0s 872us/step - loss: 0.0010 - val_loss: 0.1083\n",
            "Epoch 245/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 0.0011 - val_loss: 0.1080\n",
            "Epoch 246/500\n",
            "65/65 [==============================] - 0s 846us/step - loss: 0.0010 - val_loss: 0.1076\n",
            "Epoch 247/500\n",
            "65/65 [==============================] - 0s 851us/step - loss: 0.0010 - val_loss: 0.1075\n",
            "Epoch 248/500\n",
            "65/65 [==============================] - 0s 869us/step - loss: 9.9181e-04 - val_loss: 0.1074\n",
            "Epoch 249/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.1074\n",
            "Epoch 250/500\n",
            "65/65 [==============================] - 0s 872us/step - loss: 9.3510e-04 - val_loss: 0.1073\n",
            "Epoch 251/500\n",
            "65/65 [==============================] - 0s 849us/step - loss: 8.6594e-04 - val_loss: 0.1074\n",
            "Epoch 252/500\n",
            "65/65 [==============================] - 0s 848us/step - loss: 0.0010 - val_loss: 0.1075\n",
            "Epoch 253/500\n",
            "65/65 [==============================] - 0s 862us/step - loss: 9.3975e-04 - val_loss: 0.1078\n",
            "Epoch 254/500\n",
            "65/65 [==============================] - 0s 871us/step - loss: 9.7531e-04 - val_loss: 0.1080\n",
            "Epoch 255/500\n",
            "65/65 [==============================] - 0s 866us/step - loss: 0.0010 - val_loss: 0.1083\n",
            "Epoch 256/500\n",
            "65/65 [==============================] - 0s 918us/step - loss: 0.0011 - val_loss: 0.1085\n",
            "Epoch 257/500\n",
            "65/65 [==============================] - 0s 874us/step - loss: 9.6450e-04 - val_loss: 0.1086\n",
            "Epoch 258/500\n",
            "65/65 [==============================] - 0s 903us/step - loss: 0.0010 - val_loss: 0.1086\n",
            "Epoch 259/500\n",
            "65/65 [==============================] - 0s 868us/step - loss: 0.0010 - val_loss: 0.1085\n",
            "Epoch 260/500\n",
            "65/65 [==============================] - 0s 884us/step - loss: 9.6713e-04 - val_loss: 0.1081\n",
            "Epoch 261/500\n",
            "65/65 [==============================] - 0s 971us/step - loss: 9.1278e-04 - val_loss: 0.1078\n",
            "Epoch 262/500\n",
            "65/65 [==============================] - 0s 972us/step - loss: 0.0010 - val_loss: 0.1074\n",
            "Epoch 263/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.6492e-04 - val_loss: 0.1070\n",
            "Epoch 264/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.5654e-04 - val_loss: 0.1065\n",
            "Epoch 265/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.8935e-04 - val_loss: 0.1059\n",
            "Epoch 266/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.3197e-04 - val_loss: 0.1053\n",
            "Epoch 267/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.1049\n",
            "Epoch 268/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.1045\n",
            "Epoch 269/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.9933e-04 - val_loss: 0.1043\n",
            "Epoch 270/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.9018e-04 - val_loss: 0.1042\n",
            "Epoch 271/500\n",
            "65/65 [==============================] - 0s 975us/step - loss: 9.6969e-04 - val_loss: 0.1040\n",
            "Epoch 272/500\n",
            "65/65 [==============================] - 0s 937us/step - loss: 0.0010 - val_loss: 0.1036\n",
            "Epoch 273/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.1033\n",
            "Epoch 274/500\n",
            "65/65 [==============================] - 0s 974us/step - loss: 9.4653e-04 - val_loss: 0.1032\n",
            "Epoch 275/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.1031\n",
            "Epoch 276/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.7271e-04 - val_loss: 0.1031\n",
            "Epoch 277/500\n",
            "65/65 [==============================] - 0s 892us/step - loss: 9.8274e-04 - val_loss: 0.1032\n",
            "Epoch 278/500\n",
            "65/65 [==============================] - 0s 786us/step - loss: 8.9666e-04 - val_loss: 0.1032\n",
            "Epoch 279/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 9.5989e-04 - val_loss: 0.1033\n",
            "Epoch 280/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.2943e-04 - val_loss: 0.1031\n",
            "Epoch 281/500\n",
            "65/65 [==============================] - 0s 936us/step - loss: 0.0010 - val_loss: 0.1028\n",
            "Epoch 282/500\n",
            "65/65 [==============================] - 0s 854us/step - loss: 9.2788e-04 - val_loss: 0.1027\n",
            "Epoch 283/500\n",
            "65/65 [==============================] - 0s 883us/step - loss: 0.0011 - val_loss: 0.1028\n",
            "Epoch 284/500\n",
            "65/65 [==============================] - 0s 849us/step - loss: 0.0010 - val_loss: 0.1029\n",
            "Epoch 285/500\n",
            "65/65 [==============================] - 0s 890us/step - loss: 9.3107e-04 - val_loss: 0.1029\n",
            "Epoch 286/500\n",
            "65/65 [==============================] - 0s 935us/step - loss: 0.0011 - val_loss: 0.1029\n",
            "Epoch 287/500\n",
            "65/65 [==============================] - 0s 890us/step - loss: 0.0011 - val_loss: 0.1030\n",
            "Epoch 288/500\n",
            "65/65 [==============================] - 0s 865us/step - loss: 0.0011 - val_loss: 0.1029\n",
            "Epoch 289/500\n",
            "65/65 [==============================] - 0s 883us/step - loss: 9.9722e-04 - val_loss: 0.1027\n",
            "Epoch 290/500\n",
            "65/65 [==============================] - 0s 866us/step - loss: 9.6724e-04 - val_loss: 0.1025\n",
            "Epoch 291/500\n",
            "65/65 [==============================] - 0s 863us/step - loss: 0.0010 - val_loss: 0.1022\n",
            "Epoch 292/500\n",
            "65/65 [==============================] - 0s 902us/step - loss: 8.9669e-04 - val_loss: 0.1021\n",
            "Epoch 293/500\n",
            "65/65 [==============================] - 0s 862us/step - loss: 9.4046e-04 - val_loss: 0.1019\n",
            "Epoch 294/500\n",
            "65/65 [==============================] - 0s 875us/step - loss: 9.0388e-04 - val_loss: 0.1017\n",
            "Epoch 295/500\n",
            "65/65 [==============================] - 0s 1000us/step - loss: 0.0010 - val_loss: 0.1015\n",
            "Epoch 296/500\n",
            "65/65 [==============================] - 0s 894us/step - loss: 8.7440e-04 - val_loss: 0.1013\n",
            "Epoch 297/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.8918e-04 - val_loss: 0.1013\n",
            "Epoch 298/500\n",
            "65/65 [==============================] - 0s 852us/step - loss: 9.8828e-04 - val_loss: 0.1012\n",
            "Epoch 299/500\n",
            "65/65 [==============================] - 0s 858us/step - loss: 9.3823e-04 - val_loss: 0.1014\n",
            "Epoch 300/500\n",
            "65/65 [==============================] - 0s 956us/step - loss: 9.3735e-04 - val_loss: 0.1014\n",
            "Epoch 301/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.4740e-04 - val_loss: 0.1013\n",
            "Epoch 302/500\n",
            "65/65 [==============================] - 0s 887us/step - loss: 9.8891e-04 - val_loss: 0.1011\n",
            "Epoch 303/500\n",
            "65/65 [==============================] - 0s 857us/step - loss: 9.7686e-04 - val_loss: 0.1010\n",
            "Epoch 304/500\n",
            "65/65 [==============================] - 0s 863us/step - loss: 9.4497e-04 - val_loss: 0.1009\n",
            "Epoch 305/500\n",
            "65/65 [==============================] - 0s 875us/step - loss: 0.0011 - val_loss: 0.1008\n",
            "Epoch 306/500\n",
            "65/65 [==============================] - 0s 880us/step - loss: 8.6917e-04 - val_loss: 0.1009\n",
            "Epoch 307/500\n",
            "65/65 [==============================] - 0s 858us/step - loss: 0.0010 - val_loss: 0.1010\n",
            "Epoch 308/500\n",
            "65/65 [==============================] - 0s 931us/step - loss: 9.8257e-04 - val_loss: 0.1011\n",
            "Epoch 309/500\n",
            "65/65 [==============================] - 0s 852us/step - loss: 9.4413e-04 - val_loss: 0.1011\n",
            "Epoch 310/500\n",
            "65/65 [==============================] - 0s 884us/step - loss: 9.7386e-04 - val_loss: 0.1011\n",
            "Epoch 311/500\n",
            "65/65 [==============================] - 0s 875us/step - loss: 9.7930e-04 - val_loss: 0.1012\n",
            "Epoch 312/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 9.6529e-04 - val_loss: 0.1010\n",
            "Epoch 313/500\n",
            "65/65 [==============================] - 0s 900us/step - loss: 9.1171e-04 - val_loss: 0.1006\n",
            "Epoch 314/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.1741e-04 - val_loss: 0.1002\n",
            "Epoch 315/500\n",
            "65/65 [==============================] - 0s 882us/step - loss: 8.9911e-04 - val_loss: 0.0999\n",
            "Epoch 316/500\n",
            "65/65 [==============================] - 0s 939us/step - loss: 9.3235e-04 - val_loss: 0.0996\n",
            "Epoch 317/500\n",
            "65/65 [==============================] - 0s 899us/step - loss: 9.9894e-04 - val_loss: 0.0995\n",
            "Epoch 318/500\n",
            "65/65 [==============================] - 0s 905us/step - loss: 9.3856e-04 - val_loss: 0.0994\n",
            "Epoch 319/500\n",
            "65/65 [==============================] - 0s 894us/step - loss: 8.6477e-04 - val_loss: 0.0993\n",
            "Epoch 320/500\n",
            "65/65 [==============================] - 0s 847us/step - loss: 9.6911e-04 - val_loss: 0.0992\n",
            "Epoch 321/500\n",
            "65/65 [==============================] - 0s 868us/step - loss: 9.4819e-04 - val_loss: 0.0992\n",
            "Epoch 322/500\n",
            "65/65 [==============================] - 0s 967us/step - loss: 8.3177e-04 - val_loss: 0.0992\n",
            "Epoch 323/500\n",
            "65/65 [==============================] - 0s 934us/step - loss: 0.0010 - val_loss: 0.0992\n",
            "Epoch 324/500\n",
            "65/65 [==============================] - 0s 889us/step - loss: 9.7448e-04 - val_loss: 0.0992\n",
            "Epoch 325/500\n",
            "65/65 [==============================] - 0s 906us/step - loss: 9.4115e-04 - val_loss: 0.0992\n",
            "Epoch 326/500\n",
            "65/65 [==============================] - 0s 885us/step - loss: 0.0010 - val_loss: 0.0992\n",
            "Epoch 327/500\n",
            "65/65 [==============================] - 0s 883us/step - loss: 9.0179e-04 - val_loss: 0.0994\n",
            "Epoch 328/500\n",
            "65/65 [==============================] - 0s 884us/step - loss: 0.0010 - val_loss: 0.0996\n",
            "Epoch 329/500\n",
            "65/65 [==============================] - 0s 886us/step - loss: 9.4192e-04 - val_loss: 0.0998\n",
            "Epoch 330/500\n",
            "65/65 [==============================] - 0s 848us/step - loss: 9.8248e-04 - val_loss: 0.0996\n",
            "Epoch 331/500\n",
            "65/65 [==============================] - 0s 892us/step - loss: 9.8988e-04 - val_loss: 0.0995\n",
            "Epoch 332/500\n",
            "65/65 [==============================] - 0s 970us/step - loss: 9.1025e-04 - val_loss: 0.0993\n",
            "Epoch 333/500\n",
            "65/65 [==============================] - 0s 845us/step - loss: 0.0011 - val_loss: 0.0990\n",
            "Epoch 334/500\n",
            "65/65 [==============================] - 0s 876us/step - loss: 9.2663e-04 - val_loss: 0.0987\n",
            "Epoch 335/500\n",
            "65/65 [==============================] - 0s 874us/step - loss: 8.9908e-04 - val_loss: 0.0985\n",
            "Epoch 336/500\n",
            "65/65 [==============================] - 0s 874us/step - loss: 0.0010 - val_loss: 0.0981\n",
            "Epoch 337/500\n",
            "65/65 [==============================] - 0s 883us/step - loss: 0.0010 - val_loss: 0.0976\n",
            "Epoch 338/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 9.0898e-04 - val_loss: 0.0972\n",
            "Epoch 339/500\n",
            "65/65 [==============================] - 0s 983us/step - loss: 9.1841e-04 - val_loss: 0.0967\n",
            "Epoch 340/500\n",
            "65/65 [==============================] - 0s 974us/step - loss: 0.0010 - val_loss: 0.0963\n",
            "Epoch 341/500\n",
            "65/65 [==============================] - 0s 915us/step - loss: 9.2372e-04 - val_loss: 0.0959\n",
            "Epoch 342/500\n",
            "65/65 [==============================] - 0s 850us/step - loss: 9.1539e-04 - val_loss: 0.0955\n",
            "Epoch 343/500\n",
            "65/65 [==============================] - 0s 875us/step - loss: 9.4356e-04 - val_loss: 0.0954\n",
            "Epoch 344/500\n",
            "65/65 [==============================] - 0s 899us/step - loss: 0.0010 - val_loss: 0.0953\n",
            "Epoch 345/500\n",
            "65/65 [==============================] - 0s 880us/step - loss: 9.4013e-04 - val_loss: 0.0952\n",
            "Epoch 346/500\n",
            "65/65 [==============================] - 0s 903us/step - loss: 9.3151e-04 - val_loss: 0.0950\n",
            "Epoch 347/500\n",
            "65/65 [==============================] - 0s 931us/step - loss: 8.2766e-04 - val_loss: 0.0949\n",
            "Epoch 348/500\n",
            "65/65 [==============================] - 0s 889us/step - loss: 0.0010 - val_loss: 0.0951\n",
            "Epoch 349/500\n",
            "65/65 [==============================] - 0s 966us/step - loss: 9.8455e-04 - val_loss: 0.0952\n",
            "Epoch 350/500\n",
            "65/65 [==============================] - 0s 863us/step - loss: 9.7082e-04 - val_loss: 0.0954\n",
            "Epoch 351/500\n",
            "65/65 [==============================] - 0s 857us/step - loss: 8.5897e-04 - val_loss: 0.0956\n",
            "Epoch 352/500\n",
            "65/65 [==============================] - 0s 832us/step - loss: 0.0011 - val_loss: 0.0957\n",
            "Epoch 353/500\n",
            "65/65 [==============================] - 0s 836us/step - loss: 9.6315e-04 - val_loss: 0.0957\n",
            "Epoch 354/500\n",
            "65/65 [==============================] - 0s 855us/step - loss: 9.0368e-04 - val_loss: 0.0957\n",
            "Epoch 355/500\n",
            "65/65 [==============================] - 0s 849us/step - loss: 9.4820e-04 - val_loss: 0.0958\n",
            "Epoch 356/500\n",
            "65/65 [==============================] - 0s 849us/step - loss: 9.3700e-04 - val_loss: 0.0958\n",
            "Epoch 357/500\n",
            "65/65 [==============================] - 0s 850us/step - loss: 9.5072e-04 - val_loss: 0.0958\n",
            "Epoch 358/500\n",
            "65/65 [==============================] - 0s 862us/step - loss: 9.8480e-04 - val_loss: 0.0958\n",
            "Epoch 359/500\n",
            "65/65 [==============================] - 0s 849us/step - loss: 9.2220e-04 - val_loss: 0.0957\n",
            "Epoch 360/500\n",
            "65/65 [==============================] - 0s 875us/step - loss: 9.4729e-04 - val_loss: 0.0956\n",
            "Epoch 361/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 8.6996e-04 - val_loss: 0.0958\n",
            "Epoch 362/500\n",
            "65/65 [==============================] - 0s 860us/step - loss: 9.7156e-04 - val_loss: 0.0961\n",
            "Epoch 363/500\n",
            "65/65 [==============================] - 0s 881us/step - loss: 9.8911e-04 - val_loss: 0.0962\n",
            "Epoch 364/500\n",
            "65/65 [==============================] - 0s 881us/step - loss: 0.0010 - val_loss: 0.0961\n",
            "Epoch 365/500\n",
            "65/65 [==============================] - 0s 980us/step - loss: 9.4470e-04 - val_loss: 0.0959\n",
            "Epoch 366/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.6421e-04 - val_loss: 0.0954\n",
            "Epoch 367/500\n",
            "65/65 [==============================] - 0s 883us/step - loss: 9.3434e-04 - val_loss: 0.0949\n",
            "Epoch 368/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 9.5849e-04 - val_loss: 0.0944\n",
            "Epoch 369/500\n",
            "65/65 [==============================] - 0s 865us/step - loss: 8.6814e-04 - val_loss: 0.0941\n",
            "Epoch 370/500\n",
            "65/65 [==============================] - 0s 880us/step - loss: 9.5941e-04 - val_loss: 0.0938\n",
            "Epoch 371/500\n",
            "65/65 [==============================] - 0s 865us/step - loss: 9.0540e-04 - val_loss: 0.0934\n",
            "Epoch 372/500\n",
            "65/65 [==============================] - 0s 882us/step - loss: 9.9590e-04 - val_loss: 0.0931\n",
            "Epoch 373/500\n",
            "65/65 [==============================] - 0s 878us/step - loss: 9.7812e-04 - val_loss: 0.0930\n",
            "Epoch 374/500\n",
            "65/65 [==============================] - 0s 952us/step - loss: 0.0010 - val_loss: 0.0928\n",
            "Epoch 375/500\n",
            "65/65 [==============================] - 0s 961us/step - loss: 9.2728e-04 - val_loss: 0.0928\n",
            "Epoch 376/500\n",
            "65/65 [==============================] - 0s 862us/step - loss: 8.1193e-04 - val_loss: 0.0928\n",
            "Epoch 377/500\n",
            "65/65 [==============================] - 0s 883us/step - loss: 8.4746e-04 - val_loss: 0.0929\n",
            "Epoch 378/500\n",
            "65/65 [==============================] - 0s 922us/step - loss: 9.6221e-04 - val_loss: 0.0930\n",
            "Epoch 379/500\n",
            "65/65 [==============================] - 0s 882us/step - loss: 9.0279e-04 - val_loss: 0.0929\n",
            "Epoch 380/500\n",
            "65/65 [==============================] - 0s 955us/step - loss: 0.0010 - val_loss: 0.0931\n",
            "Epoch 381/500\n",
            "65/65 [==============================] - 0s 903us/step - loss: 9.9367e-04 - val_loss: 0.0931\n",
            "Epoch 382/500\n",
            "65/65 [==============================] - 0s 937us/step - loss: 8.7979e-04 - val_loss: 0.0932\n",
            "Epoch 383/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.9934e-04 - val_loss: 0.0934\n",
            "Epoch 384/500\n",
            "65/65 [==============================] - 0s 848us/step - loss: 8.6815e-04 - val_loss: 0.0934\n",
            "Epoch 385/500\n",
            "65/65 [==============================] - 0s 858us/step - loss: 9.3522e-04 - val_loss: 0.0933\n",
            "Epoch 386/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 0.0011 - val_loss: 0.0931\n",
            "Epoch 387/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 9.7015e-04 - val_loss: 0.0930\n",
            "Epoch 388/500\n",
            "65/65 [==============================] - 0s 853us/step - loss: 9.1469e-04 - val_loss: 0.0927\n",
            "Epoch 389/500\n",
            "65/65 [==============================] - 0s 859us/step - loss: 9.5665e-04 - val_loss: 0.0924\n",
            "Epoch 390/500\n",
            "65/65 [==============================] - 0s 884us/step - loss: 9.1555e-04 - val_loss: 0.0921\n",
            "Epoch 391/500\n",
            "65/65 [==============================] - 0s 873us/step - loss: 0.0010 - val_loss: 0.0917\n",
            "Epoch 392/500\n",
            "65/65 [==============================] - 0s 900us/step - loss: 8.7507e-04 - val_loss: 0.0916\n",
            "Epoch 393/500\n",
            "65/65 [==============================] - 0s 885us/step - loss: 9.4683e-04 - val_loss: 0.0914\n",
            "Epoch 394/500\n",
            "65/65 [==============================] - 0s 950us/step - loss: 9.7659e-04 - val_loss: 0.0910\n",
            "Epoch 395/500\n",
            "65/65 [==============================] - 0s 918us/step - loss: 8.7897e-04 - val_loss: 0.0908\n",
            "Epoch 396/500\n",
            "65/65 [==============================] - 0s 915us/step - loss: 0.0011 - val_loss: 0.0907\n",
            "Epoch 397/500\n",
            "65/65 [==============================] - 0s 913us/step - loss: 8.4357e-04 - val_loss: 0.0905\n",
            "Epoch 398/500\n",
            "65/65 [==============================] - 0s 891us/step - loss: 8.2483e-04 - val_loss: 0.0904\n",
            "Epoch 399/500\n",
            "65/65 [==============================] - 0s 847us/step - loss: 8.6607e-04 - val_loss: 0.0903\n",
            "Epoch 400/500\n",
            "65/65 [==============================] - 0s 927us/step - loss: 9.4476e-04 - val_loss: 0.0901\n",
            "Epoch 401/500\n",
            "65/65 [==============================] - 0s 880us/step - loss: 9.4829e-04 - val_loss: 0.0898\n",
            "Epoch 402/500\n",
            "65/65 [==============================] - 0s 851us/step - loss: 0.0010 - val_loss: 0.0896\n",
            "Epoch 403/500\n",
            "65/65 [==============================] - 0s 964us/step - loss: 8.8190e-04 - val_loss: 0.0894\n",
            "Epoch 404/500\n",
            "65/65 [==============================] - 0s 901us/step - loss: 8.5303e-04 - val_loss: 0.0894\n",
            "Epoch 405/500\n",
            "65/65 [==============================] - 0s 972us/step - loss: 8.4436e-04 - val_loss: 0.0896\n",
            "Epoch 406/500\n",
            "65/65 [==============================] - 0s 873us/step - loss: 9.4296e-04 - val_loss: 0.0900\n",
            "Epoch 407/500\n",
            "65/65 [==============================] - 0s 861us/step - loss: 8.6962e-04 - val_loss: 0.0902\n",
            "Epoch 408/500\n",
            "65/65 [==============================] - 0s 882us/step - loss: 8.9350e-04 - val_loss: 0.0902\n",
            "Epoch 409/500\n",
            "65/65 [==============================] - 0s 869us/step - loss: 9.5605e-04 - val_loss: 0.0902\n",
            "Epoch 410/500\n",
            "65/65 [==============================] - 0s 913us/step - loss: 8.9261e-04 - val_loss: 0.0899\n",
            "Epoch 411/500\n",
            "65/65 [==============================] - 0s 881us/step - loss: 9.5903e-04 - val_loss: 0.0895\n",
            "Epoch 412/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 9.2573e-04 - val_loss: 0.0892\n",
            "Epoch 413/500\n",
            "65/65 [==============================] - 0s 875us/step - loss: 9.3526e-04 - val_loss: 0.0889\n",
            "Epoch 414/500\n",
            "65/65 [==============================] - 0s 876us/step - loss: 9.5101e-04 - val_loss: 0.0884\n",
            "Epoch 415/500\n",
            "65/65 [==============================] - 0s 883us/step - loss: 8.7200e-04 - val_loss: 0.0881\n",
            "Epoch 416/500\n",
            "65/65 [==============================] - 0s 908us/step - loss: 8.9339e-04 - val_loss: 0.0878\n",
            "Epoch 417/500\n",
            "65/65 [==============================] - 0s 928us/step - loss: 9.5421e-04 - val_loss: 0.0874\n",
            "Epoch 418/500\n",
            "65/65 [==============================] - 0s 992us/step - loss: 9.1960e-04 - val_loss: 0.0870\n",
            "Epoch 419/500\n",
            "65/65 [==============================] - 0s 926us/step - loss: 7.8868e-04 - val_loss: 0.0868\n",
            "Epoch 420/500\n",
            "65/65 [==============================] - 0s 901us/step - loss: 0.0011 - val_loss: 0.0865\n",
            "Epoch 421/500\n",
            "65/65 [==============================] - 0s 874us/step - loss: 9.9407e-04 - val_loss: 0.0860\n",
            "Epoch 422/500\n",
            "65/65 [==============================] - 0s 851us/step - loss: 8.9901e-04 - val_loss: 0.0855\n",
            "Epoch 423/500\n",
            "65/65 [==============================] - 0s 845us/step - loss: 8.9749e-04 - val_loss: 0.0851\n",
            "Epoch 424/500\n",
            "65/65 [==============================] - 0s 861us/step - loss: 8.7679e-04 - val_loss: 0.0849\n",
            "Epoch 425/500\n",
            "65/65 [==============================] - 0s 855us/step - loss: 8.4851e-04 - val_loss: 0.0846\n",
            "Epoch 426/500\n",
            "65/65 [==============================] - 0s 864us/step - loss: 8.6412e-04 - val_loss: 0.0844\n",
            "Epoch 427/500\n",
            "65/65 [==============================] - 0s 853us/step - loss: 8.3678e-04 - val_loss: 0.0843\n",
            "Epoch 428/500\n",
            "65/65 [==============================] - 0s 858us/step - loss: 9.3158e-04 - val_loss: 0.0842\n",
            "Epoch 429/500\n",
            "65/65 [==============================] - 0s 853us/step - loss: 8.8851e-04 - val_loss: 0.0844\n",
            "Epoch 430/500\n",
            "65/65 [==============================] - 0s 937us/step - loss: 8.5150e-04 - val_loss: 0.0843\n",
            "Epoch 431/500\n",
            "65/65 [==============================] - 0s 883us/step - loss: 9.1642e-04 - val_loss: 0.0843\n",
            "Epoch 432/500\n",
            "65/65 [==============================] - 0s 940us/step - loss: 9.5581e-04 - val_loss: 0.0841\n",
            "Epoch 433/500\n",
            "65/65 [==============================] - 0s 882us/step - loss: 9.4117e-04 - val_loss: 0.0842\n",
            "Epoch 434/500\n",
            "65/65 [==============================] - 0s 874us/step - loss: 8.7149e-04 - val_loss: 0.0842\n",
            "Epoch 435/500\n",
            "65/65 [==============================] - 0s 962us/step - loss: 9.4985e-04 - val_loss: 0.0841\n",
            "Epoch 436/500\n",
            "65/65 [==============================] - 0s 873us/step - loss: 7.4922e-04 - val_loss: 0.0840\n",
            "Epoch 437/500\n",
            "65/65 [==============================] - 0s 935us/step - loss: 9.5715e-04 - val_loss: 0.0840\n",
            "Epoch 438/500\n",
            "65/65 [==============================] - 0s 861us/step - loss: 9.2592e-04 - val_loss: 0.0839\n",
            "Epoch 439/500\n",
            "65/65 [==============================] - 0s 889us/step - loss: 8.7270e-04 - val_loss: 0.0840\n",
            "Epoch 440/500\n",
            "65/65 [==============================] - 0s 826us/step - loss: 8.5697e-04 - val_loss: 0.0840\n",
            "Epoch 441/500\n",
            "65/65 [==============================] - 0s 886us/step - loss: 7.3668e-04 - val_loss: 0.0842\n",
            "Epoch 442/500\n",
            "65/65 [==============================] - 0s 870us/step - loss: 8.8132e-04 - val_loss: 0.0843\n",
            "Epoch 443/500\n",
            "65/65 [==============================] - 0s 846us/step - loss: 8.1074e-04 - val_loss: 0.0844\n",
            "Epoch 444/500\n",
            "65/65 [==============================] - 0s 866us/step - loss: 8.9258e-04 - val_loss: 0.0843\n",
            "Epoch 445/500\n",
            "65/65 [==============================] - 0s 921us/step - loss: 8.1569e-04 - val_loss: 0.0844\n",
            "Epoch 446/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.8585e-04 - val_loss: 0.0846\n",
            "Epoch 447/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.8412e-04 - val_loss: 0.0845\n",
            "Epoch 448/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.6459e-04 - val_loss: 0.0845\n",
            "Epoch 449/500\n",
            "65/65 [==============================] - 0s 997us/step - loss: 9.0161e-04 - val_loss: 0.0842\n",
            "Epoch 450/500\n",
            "65/65 [==============================] - 0s 996us/step - loss: 9.2252e-04 - val_loss: 0.0840\n",
            "Epoch 451/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.8944e-04 - val_loss: 0.0837\n",
            "Epoch 452/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.0466e-04 - val_loss: 0.0833\n",
            "Epoch 453/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.5300e-04 - val_loss: 0.0829\n",
            "Epoch 454/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.1842e-04 - val_loss: 0.0825\n",
            "Epoch 455/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0821\n",
            "Epoch 456/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 7.5438e-04 - val_loss: 0.0817\n",
            "Epoch 457/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.9011e-04 - val_loss: 0.0812\n",
            "Epoch 458/500\n",
            "65/65 [==============================] - 0s 978us/step - loss: 9.7842e-04 - val_loss: 0.0806\n",
            "Epoch 459/500\n",
            "65/65 [==============================] - 0s 975us/step - loss: 9.7588e-04 - val_loss: 0.0799\n",
            "Epoch 460/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0793\n",
            "Epoch 461/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.2126e-04 - val_loss: 0.0789\n",
            "Epoch 462/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.3545e-04 - val_loss: 0.0787\n",
            "Epoch 463/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.7674e-04 - val_loss: 0.0787\n",
            "Epoch 464/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.9219e-04 - val_loss: 0.0788\n",
            "Epoch 465/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.3528e-04 - val_loss: 0.0791\n",
            "Epoch 466/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.8139e-04 - val_loss: 0.0795\n",
            "Epoch 467/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.6411e-04 - val_loss: 0.0800\n",
            "Epoch 468/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.2715e-04 - val_loss: 0.0805\n",
            "Epoch 469/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.8641e-04 - val_loss: 0.0808\n",
            "Epoch 470/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0809\n",
            "Epoch 471/500\n",
            "65/65 [==============================] - 0s 936us/step - loss: 9.0405e-04 - val_loss: 0.0808\n",
            "Epoch 472/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.1879e-04 - val_loss: 0.0806\n",
            "Epoch 473/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 7.8327e-04 - val_loss: 0.0804\n",
            "Epoch 474/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.3439e-04 - val_loss: 0.0802\n",
            "Epoch 475/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.0053e-04 - val_loss: 0.0799\n",
            "Epoch 476/500\n",
            "65/65 [==============================] - 0s 967us/step - loss: 8.9653e-04 - val_loss: 0.0800\n",
            "Epoch 477/500\n",
            "65/65 [==============================] - 0s 821us/step - loss: 8.3031e-04 - val_loss: 0.0801\n",
            "Epoch 478/500\n",
            "65/65 [==============================] - 0s 852us/step - loss: 9.6589e-04 - val_loss: 0.0802\n",
            "Epoch 479/500\n",
            "65/65 [==============================] - 0s 886us/step - loss: 8.4451e-04 - val_loss: 0.0802\n",
            "Epoch 480/500\n",
            "65/65 [==============================] - 0s 876us/step - loss: 9.3571e-04 - val_loss: 0.0802\n",
            "Epoch 481/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.1388e-04 - val_loss: 0.0802\n",
            "Epoch 482/500\n",
            "65/65 [==============================] - 0s 976us/step - loss: 9.6275e-04 - val_loss: 0.0799\n",
            "Epoch 483/500\n",
            "65/65 [==============================] - 0s 921us/step - loss: 8.3522e-04 - val_loss: 0.0795\n",
            "Epoch 484/500\n",
            "65/65 [==============================] - 0s 846us/step - loss: 9.7102e-04 - val_loss: 0.0791\n",
            "Epoch 485/500\n",
            "65/65 [==============================] - 0s 853us/step - loss: 8.8583e-04 - val_loss: 0.0789\n",
            "Epoch 486/500\n",
            "65/65 [==============================] - 0s 851us/step - loss: 8.2982e-04 - val_loss: 0.0787\n",
            "Epoch 487/500\n",
            "65/65 [==============================] - 0s 872us/step - loss: 8.7037e-04 - val_loss: 0.0784\n",
            "Epoch 488/500\n",
            "65/65 [==============================] - 0s 904us/step - loss: 8.7991e-04 - val_loss: 0.0782\n",
            "Epoch 489/500\n",
            "65/65 [==============================] - 0s 892us/step - loss: 0.0010 - val_loss: 0.0780\n",
            "Epoch 490/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.2440e-04 - val_loss: 0.0779\n",
            "Epoch 491/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.7816e-04 - val_loss: 0.0777\n",
            "Epoch 492/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.1359e-04 - val_loss: 0.0773\n",
            "Epoch 493/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.3331e-04 - val_loss: 0.0770\n",
            "Epoch 494/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.8732e-04 - val_loss: 0.0767\n",
            "Epoch 495/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.4661e-04 - val_loss: 0.0765\n",
            "Epoch 496/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.5309e-04 - val_loss: 0.0762\n",
            "Epoch 497/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 7.9271e-04 - val_loss: 0.0761\n",
            "Epoch 498/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 8.2496e-04 - val_loss: 0.0761\n",
            "Epoch 499/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.3993e-04 - val_loss: 0.0762\n",
            "Epoch 500/500\n",
            "65/65 [==============================] - 0s 1ms/step - loss: 9.9488e-04 - val_loss: 0.0765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7faf99c68cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFp5UUl6_iJ1",
        "colab_type": "text"
      },
      "source": [
        "## 6. Predict\n",
        "\n",
        "Use evaluate function to get the loss metric. Apply the created model on test dataset. \n",
        "\n",
        "Plot the predicted and actual values on a graph. \n",
        "\n",
        "The lower the 'score' metric (ie mean square error), better is our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE3X6YY1DrE3",
        "colab_type": "code",
        "outputId": "9cdfb75a-21cb-488e-ee05-d5064e961e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "score = GRU_model.evaluate(X_test_data, y_test_data) #Evaluate model can be used to get the loss metric. \n",
        "print('Score: {}'.format(score)) \n",
        "y_pred = GRU_model.predict(X_test_data) #Apply the built model on test data. \n",
        "y_test_pred = y_scaled_set.inverse_transform(y_pred) #Use inverse transform to undo the scaling that was done before. This gives us the original values. \n",
        "y_test_actual = y_scaled_set.inverse_transform(y_test_data) # Apply the inverse transform on the actual dataset for easy comparison of predicted vs actual. \n",
        "#plt.plot(y_test_pred[-100:], label='Predicted') #Plot the predicted vs actual graph\n",
        "#plt.plot(y_test_actual[-100:], label='Actual')\n",
        "#plt.legend()\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r8/8 [==============================] - 0s 836us/step\n",
            "Score: 0.008042370900511742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvDPH7PFCzRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_Dates=main_dataset.Date[80:88] #create an object which has the dates for the Test data. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzcp1Gb7F6ed",
        "colab_type": "code",
        "outputId": "2ee979d3-8e72-4486-d09c-96da40182b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.plot(test_Dates,y_test_actual[-100:], label='Actual')\n",
        "plt.plot(test_Dates,y_test_pred[-100:],label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdnZi07zBJOW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The mean square error (metric) is nearing zero. Analyzing the above graph, we see that our model was closely predicting for the 1st day. Though there are dip in cases after 20th April, our model predicted around 1600 cases. The cases saw a sudden spike which our model was able to predict very closely and maintained the trend till 27-April."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFUehLucAXDl",
        "colab_type": "text"
      },
      "source": [
        "## Ending Notes\n",
        "\n",
        "Please note the following very important points.\n",
        "\n",
        "\n",
        "\n",
        "1.   We are dealing with a real-life scenario which we haven't seen before and hence, there can be room for lot of improvement. Given the scracity of the data available, sudden spikes and sudden dips are even more difficult to predict as the time series models show a constant trend while predicting. This is a very simple attempt in explaining GRU model for predicting time-series models. \n",
        "2.   Next steps would be change the moving average and exponential moving average to different spans and check how is the accuracy. Is our model doing any better?\n",
        "3. Try changing the parameters of our GRU model and see if the model is performing better. \n",
        "\n"
      ]
    }
  ]
}